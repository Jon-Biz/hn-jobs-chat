{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations 1: Phrase frequncy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_data():\n",
    "    # Opening JSON file\n",
    "    f = open('../posts.json')\n",
    "\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    posts = json.load(f)\n",
    "\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "data = get_data()\n",
    "\n",
    "for post in data:\n",
    "    comment = post['comment']\n",
    "\n",
    "    # hashes for ngrams\n",
    "\n",
    "    hashes = [ {}, {}, {}, {}]\n",
    "\n",
    "    for datum in data:\n",
    "        comment = datum['comment']\n",
    "        comment = comment.replace('.','')\n",
    "        comment = comment.replace(',','')\n",
    "        comment = comment.replace(''|'')\n",
    "        commentArr = comment.split(' ')\n",
    "\n",
    "        for i in range(len(hashes)):\n",
    "            hashLen = i + 2\n",
    "            postHash = hashes[i]\n",
    "\n",
    "            for idx in range(len(commentArr)):\n",
    "                if idx + hashLen < len(commentArr):\n",
    "                    gram = ' '.join(commentArr[idx:idx+hashLen])\n",
    "                    if gram in postHash:\n",
    "                        postHash[gram] += 1\n",
    "                    else:\n",
    "                        postHash[gram] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in the', 66), (' ', 65), ('  ', 59), ('looking for', 55), ('   ', 53), ('    ', 47), ('Software Engineer', 46), ('Engineer |', 43), ('| Full-time', 39), ('of the', 39)]\n",
      "Most common 2 grams\n",
      "[('in the', 66), (' ', 65), ('looking for', 55), ('Software Engineer', 46), ('Engineer |', 43), ('| Full-time', 39), ('of the', 39), ('Full-time |', 36), ('is a', 36), ('for a', 32)]\n",
      "Most common 3 grams\n",
      "[('  ', 59), ('| Full-time |', 35), ('Senior Software Engineer', 20), ('to join our', 17), ('looking for a', 16), ('are looking for', 15), ('reach out to', 13), ('Software Engineer |', 11), ('| Remote |', 10), ('Engineer | Remote', 10)]\n",
      "Most common 4 grams\n",
      "[('   ', 53), ('New York NY |', 9), ('| Senior Software Engineer', 9), ('reach out to me', 7), ('on a mission to', 7), ('York NY | Austin', 7), ('NY | Austin TX', 7), ('| Austin TX |', 7), ('Austin TX | Phoenix', 7), ('are looking for a', 6)]\n",
      "Most common 5 grams\n",
      "[('    ', 47), ('New York NY | Austin', 7), ('York NY | Austin TX', 7), ('NY | Austin TX |', 7), ('| Austin TX | Phoenix', 7), ('| Remote | Full-time |', 5), ('| Senior Software Engineer |', 5), ('HYBRID - New York NY', 5), ('- New York NY |', 5), ('please reach out to me', 4)]\n"
     ]
    }
   ],
   "source": [
    "# hashes is a list of dictionaries, each dictionary contains the ngrams for that specific ngram length\n",
    "\n",
    "# We need to find the most common ngrams across all ngram lengths\n",
    "\n",
    "# We can do this by combining all the dictionaries into one and then finding the most common ngrams\n",
    "\n",
    "allHashes = {}\n",
    "\n",
    "for postHash in hashes:\n",
    "    for key in postHash:\n",
    "        if key in allHashes:\n",
    "            allHashes[key] += postHash[key]\n",
    "        else:\n",
    "            allHashes[key] = postHash[key]\n",
    "\n",
    "# Now we can find the most common ngrams\n",
    "\n",
    "mostCommon = Counter(allHashes)\n",
    "\n",
    "print(mostCommon.most_common(10))\n",
    "\n",
    "# We can also find the most common ngrams for each ngram length\n",
    "\n",
    "for i in range(len(hashes)):\n",
    "\n",
    "    print(f\"Most common {i+2} grams\")\n",
    "    mostCommon = Counter(hashes[i])\n",
    "    print(mostCommon.most_common(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
